{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary Libraries.\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import yfinance as yf\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the stock that you want to analyze.\n",
    "stock = ''\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"FINNHUB_API_KEY\")\n",
    "\n",
    "# Initialize an empty DataFrame to store all data\n",
    "all_data = pd.DataFrame()\n",
    "\n",
    "# Define a start date\n",
    "start_date = pd.to_datetime(\"2020-01-01\")\n",
    "\n",
    "while start_date < pd.to_datetime(\"today\"):\n",
    "    # Define the end date for this chunk (100 days later, for example)\n",
    "    end_date = start_date + pd.Timedelta(days=100)\n",
    "    \n",
    "    # Request for data from Finnhub.io\n",
    "    url = f'https://finnhub.io/api/v1/stock/insider-transactions?symbol={stock}&from={start_date.strftime(\"%Y-%m-%d\")}&to={end_date.strftime(\"%Y-%m-%d\")}&token={api_key}'\n",
    "\n",
    "    r = requests.get(url)\n",
    "    \n",
    "    # Load the JSON file as a string.\n",
    "    test = json.loads(r.text)\n",
    "    \n",
    "    # Convert the data into a dataframe.\n",
    "    df = pd.DataFrame(data=test['data'])\n",
    "    \n",
    "    # Derived attributes from the data.\n",
    "    df['dollarAmount'] = df['change']*df['transactionPrice']\n",
    "    df['insiderPortfolioChange'] = df['change']/(df['share'] - df['change'])\n",
    "    conditions = [\n",
    "        (df['change'] >= 0) & (df['transactionPrice'] > 0),\n",
    "        (df['change'] <= 0) & (df['transactionPrice'] > 0),\n",
    "        (df['transactionPrice'] == 0)\n",
    "    ]\n",
    "    values = ['Buy', 'Sale', 'Gift']\n",
    "    df['buyOrSale'] = np.select(conditions, values)\n",
    "    df = df[['change', 'filingDate', 'name', 'share', 'source', 'symbol', 'transactionCode', 'transactionDate', 'transactionPrice', 'dollarAmount', 'insiderPortfolioChange', 'buyOrSale']]\n",
    "    \n",
    "    # Append the data to the main DataFrame\n",
    "    all_data = pd.concat([all_data, df])\n",
    "    \n",
    "    # Move the start date to the next chunk\n",
    "    start_date = end_date + pd.Timedelta(days=1)\n",
    "    \n",
    "    # Sleep for a short duration to avoid hitting rate limits\n",
    "    time.sleep(1)\n",
    "\n",
    "all_data.reset_index(drop=True, inplace=True)\n",
    "all_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of records\n",
    "num_records = all_data.shape[0]\n",
    "print(f\"Number of Records: {num_records}\")\n",
    "\n",
    "# Number of duplicates\n",
    "num_duplicates = all_data.duplicated().sum()\n",
    "print(f\"Number of Duplicates: {num_duplicates}\")\n",
    "\n",
    "# Number of null values for each column\n",
    "null_values = all_data.isnull().sum()\n",
    "print(\"\\nNumber of Null Values for Each Column:\")\n",
    "print(null_values)\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(all_data.describe())\n",
    "\n",
    "# More detailed information:\n",
    "print(\"\\nDataFrame Info:\")\n",
    "print(all_data.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = all_data\n",
    "# Remove duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Verify duplicates are removed\n",
    "print(f\"Number of Duplicates After Removal: {df.duplicated().sum()}\")\n",
    "\n",
    "# Drop rows where 'share' is NaN\n",
    "df.dropna(subset=['share'], inplace=True)\n",
    "\n",
    "# Reset the index after dropping rows\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Filter out rows with future transaction dates\n",
    "df = df[df['transactionDate'] <= pd.Timestamp.today().strftime('%Y-%m-%d')]\n",
    "\n",
    "# Convert the 'transactionDate' column to datetime format\n",
    "df['transactionDate'] = pd.to_datetime(df['transactionDate'])\n",
    "\n",
    "# Filter out transactions that happened within the last week\n",
    "one_week_ago = pd.Timestamp.today() - pd.Timedelta(days=7)\n",
    "df = df[df['transactionDate'] <= one_week_ago]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_prices(symbol, start_date, end_date):\n",
    "    print(f\"{symbol}: looking for data between {start_date} and {end_date}\")\n",
    "    try:\n",
    "        stock_data = yf.download(symbol, start=start_date, end=end_date)\n",
    "        return stock_data['Close'].iloc[0], stock_data['Close'].iloc[-1]\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch data for {symbol}. Reason: {e}\")\n",
    "        return np.nan, np.nan\n",
    "\n",
    "def calculate_relative_performance(row):\n",
    "    # Convert the transactionDate from string to datetime\n",
    "    transaction_date = pd.to_datetime(row['transactionDate'])\n",
    "    \n",
    "    # Fetch stock and S&P 500 prices for the transaction date and 1 week later\n",
    "    stock_start, stock_end = fetch_prices(row['symbol'], transaction_date, transaction_date + pd.Timedelta(days=7))\n",
    "    sp500_start, sp500_end = fetch_prices('^GSPC', transaction_date, transaction_date + pd.Timedelta(days=7))\n",
    "    \n",
    "    # Calculate relative performance\n",
    "    stock_performance = (stock_end - stock_start) / stock_start\n",
    "    sp500_performance = (sp500_end - sp500_start) / sp500_start\n",
    "    \n",
    "    return stock_performance - sp500_performance\n",
    "\n",
    "# Apply the function to your dataframe\n",
    "df['relativePerformance'] = df.apply(calculate_relative_performance, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where 'share' is NaN\n",
    "df.dropna(subset=['relativePerformance'], inplace=True)\n",
    "df.dropna(subset=['insiderPortfolioChange'], inplace=True)\n",
    "# Reset the index after dropping rows\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "# Number of null values for each column\n",
    "null_values = df.isnull().sum()\n",
    "print(\"\\nNumber of Null Values for Each Column:\")\n",
    "print(null_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (stock == ''):\n",
    "    stock = 'all'\n",
    "\n",
    "df.to_csv('../data/'+stock+'.csv', index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
